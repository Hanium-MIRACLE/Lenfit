{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_pose_landmarks(video_path):\n",
    "    mp_pose = mp.solutions.pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    pose_landmarks = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mp_pose.process(frame)\n",
    "        if results.pose_landmarks:\n",
    "            pose_landmarks.append(results.pose_landmarks)\n",
    "\n",
    "    cap.release()\n",
    "    mp_pose.close()\n",
    "    return pose_landmarks\n",
    "\n",
    "def calculate_pose_similarity(pose_landmarks1, pose_landmarks2):\n",
    "    similarity_scores = []  # 동작 유사도를 저장하는 리스트\n",
    "    for pose1, pose2 in zip(pose_landmarks1, pose_landmarks2):\n",
    "        # 특정 랜드마크의 인덱스 추출\n",
    "        shoulder_idx = mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value\n",
    "        hip_idx = mp.solutions.pose.PoseLandmark.LEFT_HIP.value\n",
    "\n",
    "        # pose1과 pose2에서 어깨와 엉덩이의 좌표 추출\n",
    "        shoulder1 = pose1.landmark[shoulder_idx]\n",
    "        hip1 = pose1.landmark[hip_idx]\n",
    "        shoulder2 = pose2.landmark[shoulder_idx]\n",
    "        hip2 = pose2.landmark[hip_idx]\n",
    "\n",
    "        # 어깨와 엉덩이 간의 유클리디안 거리 계산\n",
    "        distance = np.linalg.norm(np.array([shoulder1.x, shoulder1.y]) - np.array([shoulder2.x, shoulder2.y])) \\\n",
    "                   + np.linalg.norm(np.array([hip1.x, hip1.y]) - np.array([hip2.x, hip2.y]))\n",
    "\n",
    "        similarity_scores.append(distance)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "# 두 개의 동영상 파일 경로\n",
    "video_path1 = 'ML/feedback/Dataset/score/video1.mp4'\n",
    "video_path2 = 'ML/feedback/Dataset/score/video2.mp4'\n",
    "\n",
    "# 포즈 추출\n",
    "pose_landmarks1 = get_pose_landmarks(video_path1)\n",
    "pose_landmarks2 = get_pose_landmarks(video_path2)\n",
    "\n",
    "# 동작 유사도 계산\n",
    "similarity_scores = calculate_pose_similarity(pose_landmarks1, pose_landmarks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "X = np.array(similarity_scores).reshape(-1, 1)\n",
    "y = np.array([1, 0])  # 예시로 두 동영상이 동일한 동작이면 1, 다른 동작이면 0으로 라벨링한 것입니다.\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 신경망 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
